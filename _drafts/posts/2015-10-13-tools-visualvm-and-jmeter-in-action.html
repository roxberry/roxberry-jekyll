---
layout: post
status: publish
published: true
title: VisualVM and JMeter in Action
author:
  display_name: Mark Roxberry
  login: admin
  email: mark@roxberries.com
  url: ''
author_login: admin
author_email: mark@roxberries.com
wordpress_id: 5991
wordpress_url: http://www.roxberry.pro/?p=5991
date: '2015-10-13 18:58:42 -0400'
date_gmt: '2015-10-13 22:58:42 -0400'
categories:
- Tools
tags:
- devops
- tools
- java
comments: []
---
<p>I spent a&nbsp;good part of the past several&nbsp;days trying to nail down a memory leak in our application stack. &nbsp;My current client hosts a few web applications in Tomcat - applications to provide REST services, a secure web portal and a war to run <a href="http:&#47;&#47;camel.apache.org" target="_blank">Apache Camel<&#47;a> routes. &nbsp;Intercommunication is accomplished using ActiveMQ and it has worked very well. &nbsp;But now it looks like we have a hole in the dam as QA is reporting unresponsive server calls. &nbsp;I started an initial watch on&nbsp;the Tomcat process using PS &#47; TOP - a rudimentary perf check. &nbsp;From this, I could get a count of threads and general memory usage, but not much more, though that was enough to make the case to isolate the server and to break out the perf tools.</p>
<p>I had&nbsp;JMX ports opened in the firewall and broke open <a href="http:&#47;&#47;visualvm.java.net" target="_blank">VisualVM<&#47;a> to do some casual observations - run the server, watch the threads and heap, think, watch the server consume itself, restart the server, think. &nbsp;The server would go up for N number of threads and then down with less than N number of threads until the server was unresponsive, killing QA productivity and making the developers look bad. &nbsp;I was able to match the increase in threads to the timing of a really busy Quartz job. &nbsp;I eliminated all but the host application for Apache Camel as culprits - as the thread count increase corresponds to the busy <a href="http:&#47;&#47;quartz-scheduler.org" target="_blank">Quartz<&#47;a> job&nbsp;that runs from that application. &nbsp;Phase 1 complete - get a sense of the problem area.</p>
<p>Looking at the code, I broke the job down into 3 operations, ran it locally and found the cause of the leak. &nbsp;The job consisted of a Camel route that had 3 sub-routes. &nbsp;The 2nd route was throwing an NPE (Null Pointer Exception), but not completing the work it started. &nbsp;The number of threads left opened roughly matched the number of NPE's the 2nd part of the job was throwing. &nbsp;I figure fix the NPE (that of course should never have happened), fix the leak. &nbsp;Done and done, commit, push,&nbsp;package, deploy. &nbsp;Phase 2 complete - fix the obvious.</p>
<p>New deployment is now in play, the now fixed, formerly&nbsp;leaky job is running. &nbsp;I start up VisualVM and monitor the activity - no thread increase, heap memory actually looks like it is optimizing itself. &nbsp;All looks good. &nbsp;I break out my old handy <a href="http:&#47;&#47;jmeter.apache.org" target="_blank">JMeter<&#47;a> thread breaker JMX and run that - 10 threads, looping forever, calling a payload heavy REST call that will keep the server occupied. &nbsp;All looks good - response times are fast and more importantly consistent - no dramatic increases. &nbsp;Phase 3 complete - confirm my fix.</p>
<p>There's a certain black art to finding and fixing leaks. &nbsp;From my experience, it is usually caused by something dopey and egregious like an NPE in a frequently scheduled job. &nbsp;Using tools like VisualVM and JMeter can help fix the dam and help you get back to work.</p>
<p><strong>UPDATE 10&#47;18<&#47;strong></p>
<p>The problem was not in fact solved. The bug was fixed and I think that bought us enough breathing room, i.e. more time between hangs, but we were still getting "java.lang.OutOfMemoryError: unable to create new native thread" when hitting over 1K threads.  So, checking configurations was my next step -</p>
<p>TOMCAT - server.xml     - changed from 500 to 1500 maxThreads<br />
CAMEL  - routes.xml     - added a threadPoolProfile and attempted to constrain threads<br />
LINUX  - 90-nproc.conf  - changed the wildcard entry from 1024 to 65535 thread <strong>so far this has been the key<&#47;strong></p>
<p>Rebooted the server, waited to when multiple jobs were active and then hit the server with a JMETER test with number of users = 200.  The server CPU went high, the used heap memory increased, the threads increased, but the server kept running. To confirm, I changed the 90-nproc.conf settings back to 1024, ran the JMETER test on the next peak job activity cycle and get the OOM error. Rinse and repeat to confirm and it is confirmed, IMHO. I leave the 90-proc.conf setting at 65535 and will be watching it through Monday morning peak activity. Now I need to drill into the 90-nproc settings to understand its purpose.</p>
